---
title: "a3_part1 and 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library('pacman')
pacman::p_load(tidyverse, tidymodels, multilevelmod, rstanarm, tidybayes, broom.mixed, conflicted, DALEX, DALEXtra, vip)


conflict_scout()

conflict_prefer('filter', 'dplyr')
conflict_prefer('lag', 'dplyr')

```



# Part II - machine learning pipeline on simulated data

On the two simulated datasets (separately) build a machine learning pipeline: i) create a data budget (e.g. balanced training and test sets); ii) pre-process the data (e.g. scaling the features); iii) fit and assess a classification algorithm on the training data (e.g. Bayesian multilevel logistic regression); iv) assess performance on the test set; v) discuss whether performance is as expected and feature importance is as expected.

Bonus question: replace the bayesian multilevel regression with a different algorithm, e.g. SVM or random forest (but really, anything you'd like to try).


1) (done) budget

2) (done*) pre-process(center, scale, etc.)
      - you can probably add some more steps
      
3) (done*) fit the model
      - you probably should visualise the predictive prior distributions (look up the lecture)
      - do convergance checks
      
4) access performance
      1. looic (not sure you have to do this one)
      2. cross-validation
      3. tests
      4. graphs comparing informed and sceptic
      
5) conclusions: is performance as expected, is feature importance as expected
  - check out the lecture for what he means here


## Budgeting the data:
```{r}

# We know that using map() for a list of 2 elements might probably be considered an overkill, but we thought it would make the code easier to read and easier modify so we can reuse it in part 3. Also, we tried to see the assignment as a learning experience, and learning to use functional programming on lists of data frames seems like something useful.


splits <- map(dfs_wide, ~ .x %>% initial_split(prop = 4/5))
  
dfs_training <- map(splits, ~ .x %>% training)
dfs_testing <- map(splits, ~ .x %>% testing)

rm(splits)
```
## Preprocessing the data
```{r}
recipes <- map(dfs_training, 
               ~ recipe(condition ~ 1 + ., data = .x) %>%
                    update_role(id, trail, new_role = 'id') %>%
                    step_normalize(all_numeric())
) 

```

## Fitting (training) the models

### Creating the models
```{r}
prior_b <- normal(location = c(rep(0, 10)), scale = c(rep(0.3, 10)))
prior_intercept <- normal(0, 1)


prior_model <- logistic_reg() %>% 
  set_engine('stan',
             prior = prior_b,
             prior_intercept = prior_intercept,
             prior_PD = T,
             cores = 3)
  
  
model <- logistic_reg() %>% 
  set_engine('stan',
             prior = prior_b,
             prior_intercept = prior_intercept,
             cores = 3)
```
### Workflows
```{r}
wflows <- map(recipes,
              ~  workflow() %>% 
                  add_model(model) %>%  
                  add_recipe(.x)
)

prior_wflows <- map(recipes, 
                       ~ workflow() %>%
                          add_model(prior_model) %>%
                          add_recipe(.x))


 
```
### Model fitting
```{r}

prior_models <- list(prior_wflows[[1]] %>% fit(dfs_training[[1]]),
                     prior_wflows[[2]] %>% fit(dfs_training[[2]])
                     ) %>% map(extract_fit_engine)


fitted <- list(wflows[[1]] %>% fit(dfs_training[[1]]),
               wflows[[2]] %>% fit(dfs_training[[2]])
               )
names(fitted) <- c('Informed', 'Sceptic')


fitted_models <- fitted %>% map(extract_fit_engine)

rm(prior_wflows)
```
### Convergance checks
```{r}

convergance_plots <- map2(
  fitted_models, 
  names(fitted_models), 
  function(.x, .y){
    list(
      plot(.x, 'trace'),
      plot(.x, 'neff'),
      plot(.x, 'rhat')
      ) %>%
    map(function(.x){.x + ggtitle(.y)})
  }
)

convergance_plots %>% print

rm(convergance_plots)
```
### Checking the priors
#### Visualising the prior distributions
```{r}
#make prior visualisation like the one on the slides (week 10)
```

#### Prior-posterior update checks
```{r}
pp_update_plot <- function(prior_model, posterior_model){
  df_draws <- 
    bind_rows(
      bind_rows(
        prior_model %>% gather_draws(`(Intercept)`),
        prior_model %>% gather_draws(`v_.*`, regex = T)
        ) %>% 
        mutate(type = 'prior'),
      
      bind_rows(
        posterior_model %>% gather_draws(`(Intercept)`),
        posterior_model %>% gather_draws(`v_.*`, regex = T)
        ) %>% 
        mutate(type = 'posterior')
      )
  
  df_draws <- df_draws %>% 
    group_by(.variable) %>% 
    mutate(upp_lim = if_else((max(.value) + min(.value)) > 0, max(.value), - min(.value)),
           low_lim = - upp_lim) %>% 
    ungroup
  
  
  
  df_draws %>%  
    ggplot(aes(x = .value, fill = type)) +
      geom_density(alpha = 0.8) +
      labs(fill = element_blank()) +
      xlim(df_draws$low_lim[[1]], df_draws$upp_lim[[1]]) +
      facet_grid(vars(df_draws$.variable)) +
      theme_minimal() +
      theme(axis.ticks.y = element_blank(), 
            axis.text.y = element_blank())
}
```

```{r fig.width=10}
pp_update_plot(prior_models[[1]], fitted_models[[1]])+
  ggtitle('Informed')
```


```{r fig.width=10}
pp_update_plot(prior_models[[2]], fitted_models[[2]])+
  ggtitle('Sceptic')

```

## Visualising the model
```{r}
#add a plot of the regression line on the log-odds scale and on the probability scale



```

## Accessing model performance

### Cross-validation
```{r}
dfs_folded <- map(dfs_training, ~ vfold_cv(.x, v = 8))



cv_data <- map2(wflows, dfs_folded, ~ fit_resamples(.x, .y, metrics = metric_set(f_meas, roc_auc)))

cv_results <- map(cv_data, ~ collect_metrics(.x) %>% 
                    mutate(upper = mean + std_err,
                           lower = mean - std_err))

cv_results <- bind_rows(
    cv_results[[1]] %>% mutate(model = 'Informed'),
    cv_results[[2]] %>% mutate(model = 'Sceptic')
  )

cv_results <- cv_results %>% 
  rename_with(.cols = everything(), ~ str_remove(.x, stringr::fixed("."))) %>% 
  mutate(metric = if_else(metric == 'f_meas', 'f1', metric))
```


```{r}
cv_results%>% 
  ggplot(aes(x = mean, y = model, xmax = upper, xmin = lower, colour = model)) +
    geom_pointrange()+
    facet_wrap(vars(metric)) +
    geom_vline(xintercept = 0.5, colour = 'darkred', linetype = 'dashed', alpha = 0.7) +
    theme_minimal() +
    coord_flip()
```


### Test data
```{r}

test_preds <- map2(fitted, dfs_training, ~ augment(.x, .y))


map2(test_preds, names(test_preds),
     ~ .x %>% 
          roc_curve(truth = condition, .pred_sz) %>% 
            autoplot + 
            ggtitle(.y)
)

```
## Conclusions (is performance and feature importance as expected)

```{r}
#without uncertanity


# come up with a better name for this one
test_results_mean_only <- map2_df(test_preds, names(test_preds),
             ~ bind_rows(
                  .x %>% roc_auc(truth = condition, .pred_sz),
                  .x %>% f_meas(truth = condition, .pred_class, beta = 1) %>% mutate(.metric = 'f1')
             ) %>% 
               mutate(Model = .y)
)






test_results_mean_only %>% 
  ggplot(aes(x = Model, y = .estimate, colour = Model)) +
    geom_point()+
    facet_wrap(vars(.metric)) +
    geom_hline(yintercept = 0.5, colour = 'darkred', linetype = 'dashed', alpha = 0.7) +
    theme_minimal()

```
```{r}
#with the uncertanity 

test_results <- tibble(draw = NULL,
                       f1 = NULL,
                       model = NULL)


for (i in seq_along(fitted_models)){
  
  m <- fitted_models[[i]]
  name <- names(fitted_models)[[i]]
  
  draws_matrix <- posterior_epred(m)
  
  roc_aucs <- map_dbl(
    draws_matrix %>% split(row(draws_matrix)),
    ~ roc_auc_vec(truth = dfs_training[[1]]$condition, estimate = .x)
    )
  
  roc_aucs <- tibble(
    value = roc_aucs,
    metric = 'roc_auc',
    draw = seq_along(nrow)
    )
  
  
  preds_class <- map(
    draws_matrix %>% split(row(draws_matrix)), 
    ~ if_else(.x < 0.5, 'sz', 'hc') %>% as_factor %>% relevel('sz')
    )
  
       
  fs <- map_dbl(
    preds_class,
    ~ f_meas_vec(truth = dfs_training[[1]]$condition, estimate = .x, beta = 1)
    )
  
  fs <- tibble(
    value = fs,
    metric = 'f1',
    draw = seq_along(nrow)
    )

  
  test_results <- bind_rows(
    test_results,
    bind_rows(fs, roc_aucs) %>% mutate(model = name)
  )
}
rm(i, m, name, draws_matrix, roc_aucs, preds_class, fs)


test_results <- test_results %>%
  mutate(value = if_else(metric == 'roc_auc', 1 - value, value))

test_results_summary <- test_results %>% 
  group_by(model, metric) %>% 
  summarise(mean = mean(value), std_err = sd(value),
            #because we're dealing the the estimates of the population parameters, the sd already is the standard error (or at least so my limited understanding goes)
            lower = mean - 1.96*std_err, 
            upper = mean + 1.96*std_err)
```


```{r}
test_results %>%
    ggplot(aes(x = model, y = value, colour = model)) +
      geom_point(alpha = 0.7) +
      geom_hline(yintercept = 0.5, color = 'darkred', linetype = 'dashed', alpha = 0.7) +
      theme_minimal() +
      facet_wrap(vars(metric))
```


```{r}

# Just realised this might actually be dumb

  # 1. mean accuracy of all draws is something very different from the accuracy of the mean linear predictor
  
  #2. Second problem is that the confidence intervals in cross-validation and test might not show the same thing - the cross   validation one shows sd of the mean accuracy for each fold divided by sqrt(number of folds) while the test shows the standard diviation of the draws themselves (you checked that and the se calculated like that and the one the functions spits out are exactly the same)
  
        # What to do about it?
          # - plot only the accuracies only for the mean + ci of final model estimates?
          # - should I just back out of the confidence intervals and do the dots like he did
                # - you then have to code the cross-validation 'by hand'


performance_data <- bind_rows(
  test_results_summary %>% mutate(type = 'test'),
  cv_results %>% mutate(type = 'cross-validation')) %>% 
  ungroup

performance_data <- performance_data %>% 
  mutate(across(where(is.character), as_factor))

glimpse(performance_data)


performance_data %>% 
    ggplot(aes(x = mean, y = model, xmin = lower, xmax = upper, colour = type)) +
      geom_pointrange(position = position_dodge(width = 0.5)) +
      geom_vline(aes(xintercept = 0.5), color = 'darkred', linetype = 'dashed', alpha = 0.7) +
      labs(y = 'F1') +
      theme_minimal()+
      coord_flip() +
      facet_wrap(vars(metric))

```
## Feature importance
```{r}
vip_simulated <- function(model, truth){
vim_df <- model %>% gather_draws(`v_.*`, regex = T)
vim_df <- map2_df(vim_df %>% group_split(.variable), truth,
                  ~ .x %>% mutate(truth = .y)
                  )

vim_df %>% 
ggplot(aes(x = .value)) +
  geom_density() +
  geom_vline(aes(xintercept = truth[[1]]), color = 'darkred', linetype = 'dashed', alpha = 0.8) +
  facet_wrap(vars(.variable), nrow = , scales = 'free_x') +
  theme_minimal()
}

vip_simulated(fitted_models[[1]], informed_pop_effects) + ggtitle('Informed')
vip_simulated(fitted_models[[2]], skeptic_pop_effects) + ggtitle('Skeptic')

#how to make v_10 appear as last? (mutating to factor before ggplot and inside facet_wrap doesn't work)

```


```{r}
vips <- map(
  c(1,2),
  ~ explain_tidymodels(
      fitted[[.x]] %>% extract_fit_parsnip,
      data = dfs_training[[.x]],
      y = dfs_training[[.x]]$condition %>% as.numeric - 1,
      label = names(fitted)[[.x]]
      )
)

map(
  vips, 
  ~ .x %>% 
    model_parts %>% 
    plot(show_boxplots = F) +
      labs(title = 'Feature importance',
         subtitle = NULL)
)

map(
  vips, 
  ~ .x %>% 
    model_profile(
      type = 'partial',
      variables = paste0('v_', seq(10))
      ) %>% 
    plot() +
      labs(title = 'Partial dependence profile')
)
```


```{r}
save.image(file = "C:/Users/PSUJ/Desktop/uni/methods iii/assignment-3-methods-iii/rdata/a3_part2.Rdata")
```