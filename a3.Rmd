---
title: "Assignment 3"
author: "study group no 8"
date: "23/11/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages(pacman)
library(pacman)
pacman::p_load(tidyverse, tidymodels, multilevelmod, rstanarm, tidybayes, broom.mixed, tidytext, conflicted, DALEX, DALEXtra)

conflict_scout() #checking any possible conflicts between packages
conflict_prefer('filter', 'dplyr') #choosing the packages to prefer if conflict arises
```
# Part I - simulating the data

Use the meta-analysis reported in Parola et al (2020), create a simulated dataset with 100 matched pairs of schizophrenia and controls, each participant producing 10 repeated measures (10 trials with their speech recorded). for each of these "recordings" (data points) produce 10 acoustic measures: 6 from the meta-analysis, 4 with just random noise. Do the same for a baseline dataset including only 10 noise variables. Tip: see the slides for the code.
```{r}
simulate_data <- function(pop_effects, n = 100, n_trails = 10, individual_sd = 1, trail_sd = 0.5, error = 0.2, seed = 1){

  set.seed(seed)
    
  tibble(
    variable = map_chr(seq_along(pop_effects), ~ paste0('v_', .)), 
    population_value = pop_effects) %>% 
  mutate(id = seq(1, n, by = 1) %>% list) %>% 
    unnest(id) %>% 
  rowwise %>%
  mutate(condition = c('sz', 'hc') %>% list,
         true_effect = rnorm(1, population_value, individual_sd) / 2,
         true_effect = c(true_effect, - true_effect) %>% list,
         trail = seq(1, n_trails, by = 1) %>% list) %>% 
    unnest(c(condition, true_effect)) %>% unnest(trail) %>% 
  rowwise %>% 
  mutate(measurment = rnorm(1, true_effect, trail_sd) %>% rnorm(1, ., error),
         across(c(variable, id, condition), as_factor)) %>% 
  relocate(c(variable, population_value), .after = condition)
}
```


```{r}
m_a_values <- c(0.253, -0.546, 0.739, -1.26, -0.155, -0.75, 1.891, 0.046)

set.seed(1)
informed_pop_effects <- c(sample(m_a_values, 6, replace = F), rep(0, 4))


skeptic_pop_effects <- rep(0, 10)
```


```{r}
dfs_long <- map(list(informed_pop_effects, skeptic_pop_effects), simulate_data)

names(dfs_long) <- c('informed', 'skeptic')

head(dfs_long[[1]])
head(dfs_long[[2]])
```
```{r}
#checking whether the simulation works fine

check <- map(list(informed_pop_effects, skeptic_pop_effects), ~ simulate_data(pop_effects = .x, n = 1000))



check[[1]] %>% 
  group_by(variable) %>% 
  summarise(mean = true_effect %>% mean, sd = true_effect %>% sd) %>% 
    mutate(true_mean = population_value, 
           true_sd = 1)

check[[1]] %>% 
  group_by(id) %>% 
  summarise(mean = measurment %>% mean, sd = measurment %>% sd) %>% 
    mutate(true_mean = true_effect)
```


```{r}
#visualising the simulated data
map(dfs_long,
    ~ .x %>% 
      ggplot(aes(x = measurment, fill = condition)) +
        geom_density()+
        facet_wrap(vars(variable)) +
        theme_minimal()
)
```

```{r}
dfs_wide <- map(dfs_long,
  ~ .x %>% 
      pivot_wider(id_cols = c(id, trail, condition),
                  names_from = variable,
                  values_from = measurment)
)
head(dfs_wide[[1]])
head(dfs_wide[[2]])
```

# Part II - machine learning pipeline on simulated data

On the two simulated datasets (separately) build a machine learning pipeline: i) create a data budget (e.g. balanced training and test sets); ii) pre-process the data (e.g. scaling the features); iii) fit and assess a classification algorithm on the training data (e.g. Bayesian multilevel logistic regression); iv) assess performance on the test set; v) discuss whether performance is as expected and feature importance is as expected.

Bonus question: replace the bayesian multilevel regression with a different algorithm, e.g. SVM or random forest (but really, anything you'd like to try).
## Budgeting the data:
```{r}

# We know that using map() for a list of 2 elements might probably be considered an overkill, but we thought it would make the code easier to read and easier modify so we can reuse it in part 3. Also, we tried to see the assignment as a learning experience, and learning to use functional programming on lists of data frames seems like something useful.


splits <- map(dfs_wide, ~ .x %>% initial_split(prop = 4/5))
  
dfs_training <- map(splits, ~ .x %>% training)
dfs_testing <- map(splits, ~ .x %>% testing)

rm(splits)
```
## Preprocessing the data
```{r}
recipes <- map(dfs_training, 
               ~ recipe(condition ~ 1 + ., data = .x) %>%
                    update_role(id, trail, new_role = 'id') %>%
                    step_normalize(all_numeric())
) 

```

## Fitting (training) the models

### Creating the models
```{r}
prior_b <- normal(location = c(rep(0, 10)), scale = c(rep(0.3, 10)))
prior_intercept <- normal(0, 1)


prior_model <- logistic_reg() %>% 
  set_engine('stan',
             prior = prior_b,
             prior_intercept = prior_intercept,
             prior_PD = T,
             cores = 3)
  
  
model <- logistic_reg() %>% 
  set_engine('stan',
             prior = prior_b,
             prior_intercept = prior_intercept,
             cores = 3)
```
### Workflows
```{r}
wflows <- map(recipes,
              ~  workflow() %>% 
                  add_model(model) %>%  
                  add_recipe(.x)
)

prior_wflows <- map(recipes, 
                       ~ workflow() %>%
                          add_model(prior_model) %>%
                          add_recipe(.x))


 
```
### Model fitting
```{r}

prior_models <- list(prior_wflows[[1]] %>% fit(dfs_training[[1]]),
                     prior_wflows[[2]] %>% fit(dfs_training[[2]])
                     ) %>% map(extract_fit_engine)


fitted <- list(wflows[[1]] %>% fit(dfs_training[[1]]),
               wflows[[2]] %>% fit(dfs_training[[2]])
               )
names(fitted) <- c('Informed', 'Sceptic')


fitted_models <- fitted %>% map(extract_fit_engine)

rm(prior_wflows)
```
### Convergance checks
```{r}

convergance_plots <- map2(
  fitted_models, 
  names(fitted_models), 
  function(.x, .y){
    list(
      plot(.x, 'trace'),
      plot(.x, 'neff'),
      plot(.x, 'rhat')
      ) %>%
    map(function(.x){.x + ggtitle(.y)})
  }
)

convergance_plots %>% print

rm(convergance_plots)
```
### Checking the priors
#### Visualising the prior distributions
#### Prior-posterior update checks
```{r}
pp_update_plot <- function(prior_model, posterior_model){
  df_draws <- 
    bind_rows(
      bind_rows(
        prior_model %>% gather_draws(`(Intercept)`),
        prior_model %>% gather_draws(`v_.*`, regex = T)
        ) %>% 
        mutate(type = 'prior'),
      
      bind_rows(
        posterior_model %>% gather_draws(`(Intercept)`),
        posterior_model %>% gather_draws(`v_.*`, regex = T)
        ) %>% 
        mutate(type = 'posterior')
      )
  
  df_draws <- df_draws %>% 
    group_by(.variable) %>% 
    mutate(upp_lim = if_else((max(.value) + min(.value)) > 0, max(.value), - min(.value)),
           low_lim = - upp_lim) %>% 
    ungroup
  
  
  
  df_draws %>%  
    ggplot(aes(x = .value, fill = type)) +
      geom_density(alpha = 0.8) +
      labs(fill = element_blank()) +
      xlim(df_draws$low_lim[[1]], df_draws$upp_lim[[1]]) +
      facet_grid(vars(df_draws$.variable)) +
      theme_minimal() +
      theme(axis.ticks.y = element_blank(), 
            axis.text.y = element_blank())
}
```

```{r fig.width=10}
pp_update_plot(prior_models[[1]], fitted_models[[1]])+
  ggtitle('Informed')
```


```{r fig.width=10}
pp_update_plot(prior_models[[2]], fitted_models[[2]])+
  ggtitle('Sceptic')

```
## Accessing model performance

### Cross-validation
```{r}
dfs_folded <- map(dfs_training, ~ vfold_cv(.x, v = 8))



cv_data <- map2(wflows, dfs_folded, ~ fit_resamples(.x, .y, metrics = metric_set(f_meas, roc_auc)))

cv_results <- map(cv_data, ~ collect_metrics(.x) %>% 
                    mutate(upper = mean + std_err,
                           lower = mean - std_err))

cv_results <- bind_rows(
    cv_results[[1]] %>% mutate(model = 'Informed'),
    cv_results[[2]] %>% mutate(model = 'Sceptic')
  )

cv_results <- cv_results %>% 
  rename_with(.cols = everything(), ~ str_remove(.x, stringr::fixed("."))) %>% 
  mutate(metric = if_else(metric == 'f_meas', 'f1', metric))
```


```{r}
cv_results%>% 
  ggplot(aes(x = mean, y = model, xmax = upper, xmin = lower, colour = model)) +
    geom_pointrange()+
    facet_wrap(vars(metric)) +
    geom_vline(xintercept = 0.5, colour = 'darkred', linetype = 'dashed', alpha = 0.7) +
    theme_minimal() +
    coord_flip()
```


### Test data
```{r}

test_preds <- map2(fitted, dfs_training, ~ augment(.x, .y))


map2(test_preds, names(test_preds),
     ~ .x %>% 
          roc_curve(truth = condition, .pred_sz) %>% 
            autoplot + 
            ggtitle(.y)
)

```
## Conclusions (is performance and feature importance as expected)

```{r}
#without uncertanity


# come up with a better name for this one
test_results_mean_only <- map2_df(test_preds, names(test_preds),
             ~ bind_rows(
                  .x %>% roc_auc(truth = condition, .pred_sz),
                  .x %>% f_meas(truth = condition, .pred_class, beta = 1) %>% mutate(.metric = 'f1')
             ) %>% 
               mutate(Model = .y)
)






test_results_mean_only %>% 
  ggplot(aes(x = Model, y = .estimate, colour = Model)) +
    geom_point()+
    facet_wrap(vars(.metric)) +
    geom_hline(yintercept = 0.5, colour = 'darkred', linetype = 'dashed', alpha = 0.7) +
    theme_minimal()

```
```{r}
#with the uncertanity 

test_results <- tibble(draw = NULL,
                       f1 = NULL,
                       model = NULL)


for (i in seq_along(fitted_models)){
  
  m <- fitted_models[[i]]
  name <- names(fitted_models)[[i]]
  
  draws_matrix <- posterior_epred(m)
  
  roc_aucs <- map_dbl(
    draws_matrix %>% split(row(draws_matrix)),
    ~ roc_auc_vec(truth = dfs_training[[1]]$condition, estimate = .x)
    )
  
  roc_aucs <- tibble(
    value = roc_aucs,
    metric = 'roc_auc',
    draw = seq_along(nrow)
    )
  
  
  preds_class <- map(
    draws_matrix %>% split(row(draws_matrix)), 
    ~ if_else(.x < 0.5, 'sz', 'hc') %>% as_factor %>% relevel('sz')
    )
  
       
  fs <- map_dbl(
    preds_class,
    ~ f_meas_vec(truth = dfs_training[[1]]$condition, estimate = .x, beta = 1)
    )
  
  fs <- tibble(
    value = fs,
    metric = 'f1',
    draw = seq_along(nrow)
    )

  
  test_results <- bind_rows(
    test_results,
    bind_rows(fs, roc_aucs) %>% mutate(model = name)
  )
}
rm(i, m, name, draws_matrix, roc_aucs, preds_class, fs)


test_results <- test_results %>%
  mutate(value = if_else(metric == 'roc_auc', 1 - value, value))

test_results_summary <- test_results %>% 
  group_by(model, metric) %>% 
  summarise(mean = mean(value), std_err = sd(value),
            #because we're dealing the the estimates of the population parameters, the sd already is the standard error (or at least so my limited understanding goes)
            lower = mean - 1.96*std_err, 
            upper = mean + 1.96*std_err)
```


```{r}
test_results %>%
    ggplot(aes(x = model, y = value, colour = model)) +
      geom_point(alpha = 0.7) +
      geom_hline(yintercept = 0.5, color = 'darkred', linetype = 'dashed', alpha = 0.7) +
      theme_minimal() +
      facet_wrap(vars(metric))
```


```{r}

# Just realised this might actually not work

  # 1. mean accuracy of all draws is something very different from the accuracy of the mean linear predictor
  
  #2. Second problem is that the confidence intervals in cross-validation and test might not show the same thing - the cross   validation one shows sd of the mean accuracy for each fold divided by sqrt(number of folds) while the test shows the standard diviation of the draws themselves (you checked that and the se calculated like that and the one the functions spits out are exactly the same)
  
        # What to do about it?
          # - plot only the accuracies only for the mean + ci of final model estimates?
          # - just back out of the confidence intervals and do all the dots for cross-validation as well
                # - you then have to code the cross-validation 'by hand'


performance_data <- bind_rows(
  test_results_summary %>% mutate(type = 'test'),
  cv_results %>% mutate(type = 'cross-validation')) %>% 
  ungroup

performance_data <- performance_data %>% 
  mutate(across(where(is.character), as_factor))

glimpse(performance_data)


performance_data %>% 
    ggplot(aes(x = mean, y = model, xmin = lower, xmax = upper, colour = type)) +
      geom_pointrange(position = position_dodge(width = 0.5)) +
      geom_vline(aes(xintercept = 0.5), color = 'darkred', linetype = 'dashed', alpha = 0.7) +
      labs(y = 'F1') +
      theme_minimal()+
      coord_flip() +
      facet_wrap(vars(metric))

```
## Feature importance
```{r}
vip_simulated <- function(model, truth){
vim_df <- model %>% gather_draws(`v_.*`, regex = T)
vim_df <- map2_df(vim_df %>% group_split(.variable), truth,
                  ~ .x %>% mutate(truth = .y)
                  )

vim_df %>% 
ggplot(aes(x = .value)) +
  geom_density() +
  geom_vline(aes(xintercept = truth[[1]]), color = 'darkred', linetype = 'dashed', alpha = 0.8) +
  facet_wrap(vars(.variable), nrow = , scales = 'free_x') +
  theme_minimal()
}

vip_simulated(fitted_models[[1]], informed_pop_effects) + ggtitle('Informed')
vip_simulated(fitted_models[[2]], skeptic_pop_effects) + ggtitle('Skeptic')
```


```{r}
vips <- map(
  c(1,2),
  ~ explain_tidymodels(
      fitted[[.x]] %>% extract_fit_parsnip,
      data = dfs_training[[.x]],
      y = dfs_training[[.x]]$condition %>% as.numeric - 1,
      label = names(fitted)[[.x]]
      )
)

map(
  vips, 
  ~ .x %>% 
    model_parts %>% 
    plot(show_boxplots = F) +
      labs(title = 'Feature importance',
         subtitle = NULL)
)

map(
  vips, 
  ~ .x %>% 
    model_profile(
      type = 'partial',
      variables = paste0('v_', seq(10))
      ) %>% 
    plot() +
      labs(title = 'Partial dependence profile')
)
```


```{r}
save.image(file = "/rdata/a3_part2.Rdata")
```
# Part III

Download the empirical dataset from brightspace and apply your ML pipeline to the new data, adjusting where needed. Warning: in the simulated dataset we only had 10 features, now you have many more! Such is the life of the ML practitioner. Consider the impact a higher number of features will have on your ML inference, and decide whether you need to cut down the number of features before running the pipeline (or alternatively expand the pipeline to add feature selection).
```{r}
rm(list = ls()) 
# removing all objects from the global environment
```
      
```{r}
data_raw <- read_csv('real_data.csv')

glimpse(data_raw)
```

```{r}
data <- data_raw %>%
  rename_with(.cols = everything(), str_to_lower) %>% 
  rename(id = patid,
         condition = diagnosis) %>% 
  mutate(across(where(is.character), str_to_lower),
         across(1:7, as_factor),
         condition = if_else(condition != 'ct', 'sz', 'hc') %>% as_factor %>% relevel('sz')) %>% 
  select(-newid)

data$language %>% summary
data$corpus %>% summary

data <- data %>% 
  select(-language)


head(data)
```
## Describing the data
### Condition
```{r}
data %>% 
  count(condition) %>% 
  mutate(pct = n / sum(n), pct = pct %>% round(2))
```

### Gender
```{r}
data %>% 
  count(gender) %>% 
  mutate(pct = n / sum(n), pct = pct %>% round(2))

data %>% 
  count(gender, condition) %>% 
  group_by(condition) %>% 
  mutate(pct = n / sum(n), pct = pct %>% round(2))
```
```{r}
data %>% 
  count(corpus) %>% 
  mutate(pct = n / sum(n), pct = pct %>% round(2))


data %>% 
  count(condition, corpus) %>% 
  group_by(condition) %>% 
  mutate(pct = n / sum(n), pct = pct %>% round(2))
```

## Modeling the data
### Budgeting
```{r}
data_background <- data %>% select(1:5)
data <- data %>% select(-c(gender, corpus))

split <- initial_split(data, prop = 4/5)

data_training <- training(split)
data_testing <- testing(split)

rm(split)
```
## Preprocessing the data
```{r}
recipes <- list()

base <- recipe(condition ~ 1 + ., data = data_training) %>%
                    update_role(id, trial, new_role = 'id') %>%
                    step_normalize(all_numeric())


recipes[[1]] <- base %>% step_corr(all_predictors())
recipes[[2]] <- base %>% step_pca(all_predictors())

names(recipes) <- c('corr', 'pca')

```
### Creating the models
```{r}
prior_b <- normal(location = 0, scale = 0.3)
prior_intercept <- normal(0, 1)


model_prior <- logistic_reg() %>% 
  set_engine('stan',
             prior = prior_b,
             prior_intercept = prior_intercept,
             prior_PD = T,
             cores = 3)
  
  
model <- logistic_reg() %>% 
  set_engine('stan',
             prior = prior_b,
             prior_intercept = prior_intercept,
             cores = 3)
```
### Workflows
```{r}
wflows <- map(recipes,
               ~ workflow() %>% 
                  add_model(model) %>% 
                  add_recipe(.x))
```
### Fitting the models
```{r}
set.seed(1)
fitted <- map(wflows,
              ~ .x %>% fit(data_training))

fitted_models <- map(fitted, extract_fit_engine)

set.seed(1)
prior_fitted <- map(recipes,
  ~ workflow() %>%
     add_model(model_prior) %>%
     add_recipe(.x) %>% 
     fit(data_training) %>% 
     extract_fit_engine()
)

```
### Convergance checks
```{r}

convergance_plots <- map2(
  fitted_models, 
  names(fitted_models), 
  function(.x, .y){
    list(
      plot(.x, 'trace', pars = '(Intercept)'),
      #think about which estimates to include and add this here
      plot(.x, 'neff'),
      plot(.x, 'rhat')
      ) %>%
    map(function(.x){.x + ggtitle(.y)})
  }
)

convergance_plots %>% print

rm(convergance_plots)

```
```{r fig.width= 12, fig.height = 10}

tidy_pca <- tidy(fitted[[2]] %>% extract_recipe, 2)

tidy_pca %>%
  filter(component %in% paste0('PC', 1:5)) %>% 
  group_by(component) %>%
  top_n(15, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(value, terms)) +
  geom_col() +
  facet_wrap(~component, scale = 'free_y') +
  scale_y_reordered() +
  theme_minimal()


#ggsave('pca_interpretation.png', height = 10, width = 12, bg = 'white')
```




```{r}

variables_corr <- get_variables(fitted_models[[1]]) %>%
    str_subset('.*__', negate = T) %>%
      #removing all the 'technical' variables (e.g. 'treedepth__', 'stepsize__')
    str_subset('(Intercept)', negate = T) 

variables_corr <- c(
  '(Intercept)',
  variables_corr %>% str_subset('mcep.*') %>% sample(3, replace = F),
  variables_corr %>% str_subset('hmpdm.*') %>% sample(3, replace = F),
  variables_corr %>% str_subset(.,'mcep.*|hmpd.*', negate = T) %>% sample(3, replace = F)
)
#drawing different variables from different types of measures to plot as a sample in the pior-posterior update plots 

variables_pca <- get_variables(fitted_models[[2]]) %>% str_subset('.*__', negate = T)
```


```{r}
pp_update_plot <- function(prior_model, posterior_model, variables){
  
  df_draws <-  bind_rows(
    prior_model %>% gather_draws(!!!syms(variables))%>% 
        mutate(type = 'prior'),
      
      posterior_model %>% gather_draws(!!!syms(variables))%>% 
        mutate(type = 'posterior')
      )
  
  df_draws <- df_draws %>% 
    group_by(.variable) %>% 
    mutate(upp_lim = if_else((max(.value) + min(.value)) > 0, max(.value), - min(.value)),
           low_lim = - upp_lim) %>% 
    ungroup
  
  
  
  df_draws %>%  
    ggplot(aes(x = .value, fill = type)) +
      geom_density(alpha = 0.7) +
      labs(fill = element_blank()) +
      xlim(df_draws$low_lim[[1]], df_draws$upp_lim[[1]]) +
      facet_grid(vars(df_draws$.variable)) +
      theme_minimal() +
      theme(axis.ticks.y = element_blank(), 
            axis.text.y = element_blank())
}
```
```{r fig.height=8}
pp_update_plot(prior_fitted[[1]], fitted_models[[1]], variables_corr)

#ggsave('pp_upadte_corr.png', height = 8, bg = 'white')

pp_update_plot(prior_fitted[[2]], fitted_models[[2]], variables_pca)

#ggsave('pp_update_pca.png', height = 8, bg = 'white')

```


```{r}
test_preds <- map(fitted, ~ augment(.x, data_training))

rocs <- map2(test_preds, names(test_preds),
     ~ .x %>% 
          roc_curve(truth = condition, .pred_sz) %>% 
            autoplot + 
            ggtitle(.y)
)
rocs
#ggsave(plot = rocs[[1]], filename = 'roc_corr.png')
#ggsave(plot = rocs[[2]], filename =  'roc_pca.png')
```
```{r}
#with the uncertanity 

test_results <- tibble(draw = NULL,
                       f1 = NULL,
                       model = NULL)


for (i in seq_along(fitted_models)){
  
  m <- fitted_models[[i]]
  name <- names(fitted_models)[[i]]
  
  draws_matrix <- posterior_epred(m)
  
  roc_aucs <- map_dbl(
    draws_matrix %>% split(row(draws_matrix)),
    ~ roc_auc_vec(truth = data_training$condition, estimate = .x)
    )
  roc_aucs <- tibble(
    value = roc_aucs,
    metric = 'roc_auc',
    draw = seq_along(nrow)
    )
  
  
  preds_class <- map(
    draws_matrix %>% split(row(draws_matrix)), 
    ~ if_else(.x < 0.5, 'sz', 'hc') %>% as_factor %>% relevel('sz')
    )
  
       
  fs <- map_dbl(
    preds_class,
    ~ f_meas_vec(truth = data_training$condition, estimate = .x)
    )
  fs <- tibble(
    value = fs,
    metric = 'f1',
    draw = seq_along(nrow)
    )

  
  test_results <- bind_rows(
    test_results,
    bind_rows(fs, roc_aucs) %>% mutate(model = name)
  )
}
rm(i, m, name, draws_matrix, roc_aucs, preds_class, fs)


test_results <- test_results %>%
  mutate(value = if_else(metric == 'roc_auc', 1 - value, value))

test_results %>%
    ggplot(aes(x = model, y = value, colour = model)) +
      geom_point(alpha = 0.7) +
      geom_hline(yintercept = 0.5, color = 'darkred', linetype = 'dashed', alpha = 0.7) +
      theme_minimal() +
      facet_wrap(vars(metric))

#ggsave('test_results.png', bg = 'white')
```
```{r}
save.image(file = "/rdata/a3_part3.Rdata")
```
